# -*- coding: utf-8 -*-
"""Text Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Ahmad10Raza/Text-Summarizer-WebApp/blob/master/notebook/Text%20Summarization.ipynb

# **Text Summarization**

Text summarization is the process of extracting the main points from a text document and presenting them in a concise and coherent manner. It can be used to shorten a document, make it more readable, or extract key information.

There are many different approaches to text summarization, each with its own strengths and weaknesses. Some of the most common approaches include:

**Extractive summarization:** This approach simply extracts the most important sentences from the document and presents them in a new order.

**Abstractive summarization:** This approach attempts to generate a new summary that captures the main points of the document in a more natural and coherent way.

**Hybrid summarization:** This approach combines elements of both extractive and abstractive summarization.

The choice of which approach to use depends on the specific task at hand. For example, extractive summarization is often used for tasks such as generating bullet points or highlights, while abstractive summarization is often used for tasks such as generating short summaries or creating news articles.

Text summarization is a challenging task, but it has a wide range of applications. It can be used in areas such as information retrieval, natural language processing, and machine learning.
"""

! pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q

! pip install --upgrade accelerate
! pip uninstall -y transformers accelerate
! pip install transformers accelerate

"""# Importing Libraries"""

from transformers import pipeline, set_seed
from datasets import load_dataset, load_from_disk
import matplotlib.pyplot as plt
from datasets import load_dataset
import pandas as pd
from datasets import load_dataset, load_metric

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

import nltk
from nltk.tokenize import sent_tokenize

from tqdm import tqdm
import torch

nltk.download("punkt")

"""## pegasus Model
"google/pegasus-cnn_dailymail" refers to a **pre-trained model for text summarization** developed by Google AI. It is based on the Pegasus neural network architecture and trained on a massive dataset of news articles and summaries from the CNN/Daily Mail website.

Here's a breakdown of what "google/pegasus-cnn_dailymail" signifies:

* **google:** Indicates the model originates from Google AI.
* **pegasus:** Refers to the underlying neural network architecture used for the model, which is specifically designed for text summarization tasks.
* **cnn_dailymail:** Specifies the dataset the model was trained on, which consists of news articles and summaries from the CNN/Daily Mail website.

Therefore, "google/pegasus-cnn_dailymail" represents a readily available pre-trained model for text summarization, enabling users to perform the following:

* **Generate summaries of text documents:** The model can be used to automatically create concise and informative summaries of any text input, such as news articles, research papers, or even emails.
* **Fine-tune for specific tasks:** The pre-trained model can be further adapted and trained on smaller, domain-specific datasets to improve its performance on particular text summarization tasks.
* **Explore and understand text summarization:** Users can leverage this model to experiment and gain insights into the workings of text summarization algorithms.

Overall, "google/pegasus-cnn_dailymail" is a valuable resource for researchers, developers, and anyone interested in applying text summarization techniques for various purposes.

"""

device = "cuda" if torch.cuda.is_available() else "cpu"
device

# Import necessary libraries
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Define device for model execution (CPU or GPU)
device = "cuda" if torch.cuda.is_available() else "cpu"

# Specify pre-trained model checkpoint
model_ckpt = "google/pegasus-cnn_dailymail"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

# Load pre-trained model for text summarization
model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)

"""## Example usage:"""

text = "This is a long and detailed text that requires a concise summary."
encoded_text = tokenizer(text, return_tensors="pt").to(device)
generated_summary = model_pegasus.generate(**encoded_text)
decoded_summary = tokenizer.decode(generated_summary[0], skip_special_tokens=True)

# Print the generated summary
print(f"Summary: {decoded_summary}")

"""# The Datasets"""

#dowload & unzip data

! wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip
! unzip summarizer-data.zip

data=load_from_disk('samsum_dataset')
data

data.shape

split_lengths = [len(data[split])for split in data]

print(f"Split lengths: {split_lengths}")
print(f"Features: {data['train'].column_names}")
print("\nDialogue:")

print(data["test"][1]["dialogue"])

print("\nSummary:")

print(data["test"][1]["summary"])

def convert_examples_to_features(example_batch):
    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )

    with tokenizer.as_target_tokenizer():
        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )

    return {
        'input_ids' : input_encodings['input_ids'],
        'attention_mask': input_encodings['attention_mask'],
        'labels': target_encodings['input_ids']
    }

data_pt = data.map(convert_examples_to_features, batched = True)

data_pt['train']

data_pt['train'][1]

from transformers import DataCollatorForSeq2Seq
seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)

from transformers import TrainingArguments, Trainer

trainer_args = TrainingArguments(
    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,
    per_device_train_batch_size=1, per_device_eval_batch_size=1,
    weight_decay=0.01, logging_steps=10,
    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,
    gradient_accumulation_steps=16
)

trainer = Trainer(model=model_pegasus, args=trainer_args,
                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,
                  train_dataset=data_pt["test"],
                  eval_dataset=data_pt["validation"])

trainer.train()

# Evaluation

def generate_batch_sized_chunks(list_of_elements, batch_size):
    """split the dataset into smaller batches that we can process simultaneously
    Yield successive batch-sized chunks from list_of_elements."""
    for i in range(0, len(list_of_elements), batch_size):
        yield list_of_elements[i : i + batch_size]



def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,
                               batch_size=16, device=device,
                               column_text="article",
                               column_summary="highlights"):
    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))
    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))

    for article_batch, target_batch in tqdm(
        zip(article_batches, target_batches), total=len(article_batches)):

        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,
                        padding="max_length", return_tensors="pt")

        summaries = model.generate(input_ids=inputs["input_ids"].to(device),
                         attention_mask=inputs["attention_mask"].to(device),
                         length_penalty=0.8, num_beams=8, max_length=128)
        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''

        # Finally, we decode the generated texts,
        # replace the  token, and add the decoded texts with the references to the metric.
        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,
                                clean_up_tokenization_spaces=True)
               for s in summaries]

        decoded_summaries = [d.replace("", " ") for d in decoded_summaries]


        metric.add_batch(predictions=decoded_summaries, references=target_batch)

    #  Finally compute and return the ROUGE scores.
    score = metric.compute()
    return score

rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_metric = load_metric('rouge')

score = calculate_metric_on_test_ds(
    data['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'
)

rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )

pd.DataFrame(rouge_dict, index = [f'pegasus'] )

"""## Save The Model"""

model_pegasus.save_pretrained("pegasus-samsum-model")

tokenizer.save_pretrained("tokenizer")

#Load

tokenizer = AutoTokenizer.from_pretrained("/content/tokenizer")

#Prediction

gen_kwargs = {"length_penalty": 0.8, "num_beams":8, "max_length": 128}



sample_text = data["test"][3]["dialogue"]

reference = data["test"][3]["summary"]

pipe = pipeline("summarization", model="pegasus-samsum-model",tokenizer=tokenizer)

##
print("Dialogue:")
print(sample_text)


print("\nReference Summary:")
print(reference)


print("\nModel Summary:")
print(pipe(sample_text, **gen_kwargs)[0]["summary_text"])

"""# **Thank You!**"""